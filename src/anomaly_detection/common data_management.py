# -*- coding: utf-8 -*-
"""ashish_data_management (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tm2P4Ied-08j4EpHrgPK1ltjdlcqx380

The data is currently stored as `.xls` files. In this notebook, we will implement some code to manipulate the data as `pandas.Dataframes` and store as more efficient `.parquet` files on disk.
"""

# import any required libraries here
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq

from google.colab import drive 
drive.mount('/content/drive')

pip install --upgrade xlrd

"""First, we need to read the `.xls` files into `pandas.Dataframes`. You can use [pandas.read_excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) for this."""

col_dict = {'OBIS Bezeichnung': 'OBISBezeh', 'Betriebsstunden' : 'Betriebsstd', 'Fehlerstunden' : 'Fehlerstd',
            'Wärmeenergie Tarif 1': 'WTarif1', 'Wärmeenergie total' : 'WTarif1', 'Durchfluss' : 'Dfluss', 'Volumen' : 'Vol',
            'Vorlauftemperatur' : 'Vorlauftmp', 'Rücklauftemperatur' : 'Rücklauftmp', 'Temperaturdifferenz' : 'TmpDiff',
            'Wärmeleistung' : 'Wleistung', 'Volumen Kanal 1' : 'VK1', 'Volumen Kanal 2' : 'VK2', 'Volumen Kanal 3' : 'VK3',
            'Fehler Flags' : 'FehlerFlags', 'WV+ Arbeit Tarif 1' : 'WV+T1', 'P Summe': 'PSum',
            'WV+ Arbeit tariflos' : 'WV+tariflos', 'WV+ Arbeit Tarif 2' : 'WV+T2', 'Gerätenummer' : 'Gnummer',
            'WV- Arbeit Tarif 1' : 'WV-T1', 'WV- Arbeit tariflos' : 'WV-tariflos', 'BV+ Arbeit Tarif 1' : 'BV+T1',
            'BV- Arbeit Tarif 1' : 'BV-T1', 'BV+ Arbeit tariflos' : 'BV+tariflos', 'BV- Arbeit tariflos' : 'BV-tariflos', 
            'WV+ Momentanwert Tariflos' : 'WV+Momenttariflos', 'Firmwareversions Nr.' : 'Fversion Nr.',
            'Impulswert Kanal 1' :'IK1', 'Impulswert Kanal 2': 'IK2', 'Impulswert Kanal 3': 'IK3',
            'Fabrikationsnummer' : 'Fabriksnummer'
}

# load the building data 
# consider the different number of header rows!
# loading data of different buildings
path = '/content/drive/MyDrive/case_studies_data/'

def data_loader(file_name1, file_name2, No_of_files):
    
    # reading files
#     df1 = pd.read_excel('case_study_data/' + file_name1, header = [2, 3, 4])
#     df2 = pd.read_excel('/case_study/case_study_data/' + file_name2, header = [2, 3, 4])
    
    if No_of_files == 1: 
        df1 = pd.read_excel(path + file_name1, header = [2, 3, 4])
        concat_df = df1.copy()
    else:
        df1 = pd.read_excel(path + file_name1, header = [2, 3, 4])
        df2 = pd.read_excel(path + file_name2, header = [2, 3, 4])
        concat_df = pd.concat([df1, df2], axis = 0) # concating data frames
#     return concat_df
    # defining Unique, friendly coulmn name ('first digit of Kennzahl' + 'from fivth digit of Beschreng' + 'Bezeichnung')
    concat_df.columns = [x[1][0] + '_' + x[0] + '_' + col_dict[x[-1]]  for x in concat_df.columns]
    
    concat_df.columns.values[0] = 'Time' # change first column nameto Time
    concat_df = concat_df.drop(0, axis = 0) # droping 1st row
    concat_df['Time'] = pd.to_datetime(concat_df['Time']) # convert to datetime format
    
    
    # converting all variables except Time to float 
    concat_df[concat_df.columns[~concat_df.columns.isin(['Time'])]] = concat_df[concat_df.columns[~concat_df.columns.isin(['Time'])]].astype('float')
    
    concat_df = concat_df.sort_values(['Time']) # sort values by time
    
   
    concat_df = concat_df.reset_index(drop = True) # reset the index and drop the previous one
#     return concat_df
    concat_df = concat_df.drop_duplicates(['Time'], keep = 'first') # dropping duplicate rows
    
    
    return concat_df

"""Next, we need to implement a function that takes a `pandas.Dataframe` and a path string as an input and writes the data to disk as a `parquet` file. You can use the [PyArrow library](https://arrow.apache.org/docs/python/parquet.html) for this: """

# calling data loader function
oh14_df = data_loader(file_name1 = 'OH14.xlsx', file_name2 = 'OH14_01_26-07_19.xls', No_of_files = 2)
oh12_df = data_loader(file_name1 = 'OH12.xls', file_name2 = 'OH12_01_26-07_19.xls', No_of_files = 2)
kita_hokida_df = data_loader(file_name1 = 'Kita Hokido.xls', file_name2 = 'Kita Hokido_05_22_20-07_19_22.xls', No_of_files = 2)
chemie_df = data_loader(file_name1 = 'Chemie.xls', file_name2 = 'Chemie_01_26-07_19.xls', No_of_files = 2)
gross_df = data_loader(file_name1 = 'Großtagespflege.xlsx', file_name2 = 'Grosstagespflege_04_05-07_19.xls', No_of_files = 2)
hg_2_df  = data_loader(file_name1 = 'HG II.xls', file_name2 = 'HGII_01_26-07_19.xls', No_of_files = 2)
ef40_df = data_loader(file_name1 = 'EF 40.xls', file_name2 = None, No_of_files = 1)
ef40a_df = data_loader(file_name1 = 'EF40a.xls', file_name2 = 'EF40a_01_22-07_19.xls', No_of_files = 2)
ef42_df = data_loader(file_name1 = 'EF 42.xls', file_name2 = None, No_of_files = 1)
erich_brost_df = data_loader(file_name1 = 'Erich-Brost-Institut.xls', file_name2 = None, No_of_files=1)

def write_as_parquet(df, path):
    # implement this function and add a short doc string describing its use
    table = pa.Table.from_pandas(df) # Construct a table from pandas dataframe
    pq.write_table(table, path) # pass this table schema to write_table function

# writing all the files in parquet format
write_as_parquet(oh14_df, path = path + '/OH14.parquet')
write_as_parquet(oh12_df, path = path + '/OH12.parquet')
write_as_parquet(kita_hokida_df, path = path + '/Kita_hokida.parquet')
write_as_parquet(chemie_df, path = path + '/Chemie.parquet')
write_as_parquet(gross_df, path = path + '/Großtagespflege.parquet')
write_as_parquet(hg_2_df, path = path + '/HGII.parquet')
write_as_parquet(ef40_df, path = path + '/EF40.parquet')
write_as_parquet(ef40a_df, path = path + '/EF40a.parquet')
write_as_parquet(ef42_df, path = path + '/EF42.parquet')
write_as_parquet(erich_brost_df, path = path + '/Erich_brost.parquet')

"""Now we need the opposite functionality: a function that reads data from a `.parquet` file on disk and returns it as a `pandas.Dataframe`. Implement this function such that it can take a list of names of column to load as an _optional_ parameter. """

def load_to_pandas(path, columns):
    # implement this function and add a short doc string describing its use
    df = pq.read_pandas(path, columns = columns).to_pandas() # reading .parquet file in pandas dataframe format
    return df

load_to_pandas(path = '/case_study/case_study_data/oh14.parquet', columns = ['Time', '6_CN37 11 01 01_WTarif1', '6_CN37 11 01 01_Dfluss'])

"""Great! We can now store data more efficiently on disk and know how to load it again. Store all the data we have as one `.parquet` file per building."""
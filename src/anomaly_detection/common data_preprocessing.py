# -*- coding: utf-8 -*-
"""example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DrucGvTkB2rGEFkDTkxHUQ1QVoShFSiK
"""

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from scipy import signal
from sklearn.preprocessing import MinMaxScaler

def _load_data(file_path):
    # tbd: load the parquet file from file_path and return as pandas DataFrame
    data = pq.read_pandas(file_path).to_pandas()
    return data

path = '/content/drive/MyDrive/case_studies_data/'

oh14_df = _load_data(file_path = path + 'OH14.parquet')
oh12_df = _load_data(file_path = path + 'OH12.parquet')
kita_hokida_df = _load_data(file_path = path + 'Kita_hokida.parquet')
chemie_df = _load_data(file_path = path + 'Chemie.parquet')
tagespflege_df = _load_data(file_path = path + 'GroÃŸtagespflege.parquet')
hg2_df = _load_data(file_path = path + 'HGII.parquet')
eh40_df = _load_data(file_path = path + 'EF40.parquet')
eh40a_df = _load_data(file_path = path + 'EF40a.parquet')
ef42_df = _load_data(file_path = path + 'EF42.parquet')
erich_brost_df = _load_data(file_path = path + 'Erich_brost.parquet')
chemie_df = _load_data(file_path = path + '/Chemie_combine.parquet')
chemie_df_s = _load_data(file_path = path + '/Chemie_singleBuilding.parquet')
office_df = _load_data(file_path = path + '/Office_combine.parquet')
office_df_s  = _load_data(file_path = path + '/Chemie_singleBuilding.parquet')


outside_temp = pd.read_csv(path + 'outside_temp.csv')
outside_temp['MESS_DATUM'] = pd.to_datetime(outside_temp['MESS_DATUM'])
outside_temp.head()

# columns for which we have to take diff
col_to_diff = ["6_CN45 11 01 01_WTarif1", "6_CN45 11 01 01_Vol", "6_CN45 21 01 01_WTarif1", "6_CN45 21 01 01_Vol",
                  "1_CN45 71 07 01_WV+T1", "1_CN45 72 22 01_WV+tariflos", "1_CN45 72 22 01_WV+T1", "1_CN45 73 01 15_WV+T1",
                  '6_CN37 11 01 01_WTarif1', '6_CN37 11 01 01_Vol', '6_CN37 21 01 02_WTarif1', '6_CN37 21 01 02_Vol',
                  '1_CN37 71 01 03_WV+T1', '8_CN37 41 01 01_VK1', '6_CN42 11 01 01_WTarif1', '8_CN42 41 01 01_VK1',
                  '1_CN42 72 01 01_WV+T1', '6_CN05 11 01 01_WTarif1', '8_CN05 41 01 03_VK1', '8_CN05 41 01 04_VK1',
                  '1_CN05 71 07 02_WV+T1', '1_CN05 72 01 01_WV+T1', '1_CN05 73 01 15_WV-T1', '8_CN05 41 05 01_VK1',
                  '1_CN05 73 01 15_WV+T1', '6_CS89 12 01 01_WTarif1', '6_CS89 12 01 01_Vol', '6_CS89 12 02 01_WTarif1',
                  '6_CS89 12 02 01_Vol', '8_CS89 41 01 02_VK1', '8_CS89 41 01 03_VK1', '1_CS89 72 01 01_WV+tariflos',
                  '1_CS89 72 01 01_WV+T1', '1_CS89 72 01 01_WV-tariflos', '1_CS89 72 01 01_WV-T1',
                  '1_CS89 72 01 02_WV-tariflos', '1_CS89 72 01 02_WV-T1', '1_CS89 72 01 01_BV-tariflos',
                  '1_CS89 72 01 01_BV-T1', '1_CS89 72 01 03_WV+tariflos', '1_CS89 72 01 03_WV+T1',
                  '1_CS89 72 01 03_BV-tariflos', '1_CS89 72 01 03_BV-T1', '1_CS89 72 01 04_WV+tariflos',
                  '1_CS89 72 01 04_WV+T1', '1_CS89 72 01 05_WV+tariflos', '1_CS89 72 01 05_WV+T1',
                  '1_CS89 72 01 06_WV-tariflos', '1_CS89 72 01 06_WV-T1', '6_CN08 12 01 13_WTarif1', '6_CN08 12 01 13_Vol',
                  '6_CN08 21 01 01_WTarif1', '6_CN08 21 01 01_Vol', '8_CN08 41 01 01_VK1', '8_CN08 41 01 02_VK1',
                  '1_CN08 63 01 01_WV+T1', '1_CN08 72 01 01_WV+T1', '1_CN08 72 10 01_WV+T1', '6_CN32 11 01 01_WTarif1',
                  '6_CN32 21 01 01_WTarif1', '1_CN32 71 03 03_WV+tariflos', '1_CN32 71 03 03_WV+T1',
                  '1_CN32 71 03 03_BV+tariflos', '1_CN32 71 03 03_BV+T1', '1_CN32 71 03 03_BV-tariflos',
                  '1_CN32 71 03 03_BV-T1', '6_CN43 11 02 01_WTarif1', '6_CN43 11 02 01_Vol', '8_CN43 41 02 01_VK1',
                  '6_CN43 11 01 01_WTarif1', '8_CN43 41 01 01_VK1', '1_CN43 71 01 03_WV+tariflos', '1_CN43 71 01 03_WV+T1',
                  '1_CN43 71 01 03_BV+tariflos', '1_CN43 71 01 03_BV+T1', '1_CN43 71 01 03_BV-tariflos',
                  '1_CN43 71 01 03_BV-T1', '6_CN33 11 01 01_WTarif1', '6_CN33 21 01 01_WTarif1', '6_CN33 21 01 01_Vol',
                  '1_CN33 71 02 03_WV+tariflos', '1_CN33 71 02 03_WV+T1', '1_CN33 71 02 03_BV+tariflos',
                  '1_CN33 71 02 03_BV+T1', '1_CN33 71 02 03_BV-tariflos', '1_CN33 71 02 03_BV-T1', '6_CN35 11 01 01_WTarif1',
                  '8_CN35 41 01 01_VK1', '1_CN35 72 01 01_WV+tariflos', '1_CN35 72 01 01_WV+T1','1_CN35 72 01 01_BV+tariflos',
                  '1_CN35 72 01 01_BV+T1', '1_CN35 72 01 01_BV-tariflos', '1_CN35 72 01 01_BV-T1', '6_CN02 12 03 42_WTarif1','6_CN02 12 03 42_Vol',
                    '6_CN02 21 03 04_WTarif1','6_CN02 21 03 04_Vol',
                    '8_CN02 43 06 33_VK1','1_CN02 72 300 01_WV+T1','1_CN02 72 301 01_WV+T1', '1_CN02 72 94 01_WV+T1',
                    '1_CN02 72 95 01_WV+tariflos', '1_CN02 72 95 01_WV+T1','1_CN02 78 300 01_WV+T1', '1_CN02 78 57 01_WV+tariflos',
                    '1_CN02 78 57 01_WV+T1', '6_CN02 12 02 49_WTarif1', '6_CN02 12 02 49_Vol', '6_CN02 21 01 03_WTarif1',
                    '6_CN02 21 01 03_Vol','8_CN02 43 06 32_VK1','1_CN02 71 03 01_WV+T1', '1_CN02 71 04 01_WV+T1',
                    '1_CN02 71 05 01_WV+T1', '1_CN02 71 07 03_WV+T1','1_CN02 72 30 01_WV+tariflos', '1_CN02 72 30 01_WV+T1',
                    '1_CN02 72 30 01_BV+tariflos','1_CN02 72 30 01_BV+T1', '1_CN02 72 30 01_BV-tariflos',
                    '1_CN02 72 30 01_BV-T1', '6_CN02 12 01 44_WTarif1', '6_CN02 12 01 44_Vol', '6_CN02 21 02 02_WTarif1',
                    '8_CN02 41 01 03_VK1', '8_CN02 41 01 04_VK1', '8_CN02 43 06 31_VK1']


def pre_function(df):

    # droping columns which has obis_code = 0
    col = [c for c in df.columns if c.lower()[0] != '0']
    df = df[col]
    
    # Drop columns that has all NaN values
    df = df.dropna(axis = 1, how = 'all')
    #print(df.shape)
    
    # droping those columns which has constant values in all rows
    col_to_drop = [col for col in df.columns.values if (
        len(df[col][df[col].notnull()].unique()) == 1)]
    df = df[df.columns[~df.columns.isin(col_to_drop)]]
    

    # Drop duplicate columns
    # Remove duplicate columns pandas DataFrame
    df2 = df.loc[:,~df.columns.duplicated()]
    #print(df2.shape)
    
    # Taking diff of columns
    col_list = df2.columns[df2.columns.isin(col_to_diff)]
    for diff_col in col_list:
        df2[diff_col].update(df2[diff_col][df2[diff_col].notnull()].diff().fillna(method ="backfill"))
#     df[col_list] = df[col_list].diff()

    # comparing two column and drop those columns which are more similar (according to threshold)
    sim_col = [] # list of similar columns
    threshold = df2.shape[0] * 0.99 # threshold 
    for col in df2.columns.values[1:]:
        for i in df2.columns[~df2.columns.isin(['Time', col])]:
            col_diff = df2[col] - df2[i]
            if (sum(col_diff == 0) > threshold): 
                if (col in sim_col) or (i in sim_col): pass
                else: sim_col.append(col)
                    
    df2 = df2[df2.columns[~df2.columns.isin(sim_col)]] # droping similar columns
    
    # creating date, day and timestamp
    df2['Date'] = df2['Time'].dt.date
    df2['Day'] = df2['Time'].dt.day
    df2['Timestamp'] = df2['Time'].dt.time
    df2['Dayofweek'] = df2['Time'].dt.dayofweek
    df2['Hourofday'] = df2['Time'].dt.hour
    
    # normalizing timestamp (00:00:00) column using min-max scaler
    df2['Timestamp'] = df2['Timestamp'].apply(lambda x:str(x).replace(':', '.')[:-3]).astype(float)
    values = df2['Timestamp'].values
    values = values.reshape((len(values), 1))
    
    scaler = MinMaxScaler()
    df2['Normalize_timestamp'] = scaler.fit_transform(values)
    #return df2
    # outside temp.
    merge_df = pd.merge(df2, outside_temp[['MESS_DATUM', 'TT_TU']], left_on=[
                        'Time'], right_on=['MESS_DATUM'], how='left')
    
    merge_df = merge_df.drop(['MESS_DATUM', 'Timestamp'], axis = 1)
    #return merge_df
    
    # converting all variables except Time to float 
    merge_df[merge_df.columns[~merge_df.columns.isin(['Time', 'Date'])]] = merge_df[merge_df.columns[~merge_df.columns.isin(['Time', 'Date'])]].astype('float')
  
    return merge_df


# train test split according to split_date
def _train_test_split(data, split_date):
    
    # split the data into training and testing sets based on the split_date parameter
    train_set = data[data['Date'] <= pd.to_datetime(split_date)]
    test_set = data[data['Date'] > pd.to_datetime(split_date)]
    
    return train_set, test_set
      

# holidays list
public_holiday_list =  ['2020-04-10','2020-04-13','2020-05-01','2020-05-21','2020-06-01','2020-06-11','2020-10-03','2020-11-01','2020-12-25','2020-12-26',
                       '2020-12-27','2020-12-28','2020-12-29','2020-12-30','2020-12-31','2021-01-01','2021-01-02','2021-01-03','2021-01-04','2021-01-05',
                       '2021-01-06','2021-01-07','2021-01-08','2021-04-02','2021-04-05','2021-05-01','2021-05-13','2021-05-24','2021-06-03','2021-10-03',
                       '2021-11-01','2021-12-25','2021-12-26','2021-12-27','2021-12-28','2021-12-29','2021-12-30','2021-12-31','2022-01-01','2022-01-02',
                       '2022-01-03','2022-01-04','2022-01-05','2022-01-06','2022-01-07','2022-04-15','2022-04-18','2022-05-01','2022-05-26','2022-06-06',
                       '2022-06-16']



def _add_artificial_label(dataset):
    # manually label the dataset and append the label as the last column
    dataset['label'] = 0
    # making labels for now by using weekend data, public holiday data and semester break
    #dataset['label'] = np.where(dataset['Dayofweek'].isin([5, 6]), 1, dataset['label'])
    #dataset['label'] = np.where(dataset['Date'].astype(str).isin(free_period_list), 1, dataset['label'])
    dataset['label'] = np.where(dataset['Date'].astype(str).isin(public_holiday_list), 1, dataset['label'])
    #dataset['label'] = np.where(((dataset['Date'] >= "2021-12-24") & (dataset['Dte'] <= "2020-01-07")), 1, dataset['label'])
    labeled_dataset = dataset.copy()
    labeled_dataset['label'] = labeled_dataset['label'].astype('float')

    return labeled_dataset

def get_data(file_path, split_date):
    data = _load_data(file_path)
    data = pre_function(data)
    labeled_dataset = _add_artificial_label(data)
    train_set, test_set = _train_test_split(labeled_dataset, split_date = split_date)
    
    return train_set, test_set

preprocessed_train_syn, preprocessed_test_syn  = get_data(file_path = path + 'EF40a..parquet', split_date = '2021-10-01')
#split_date'2021-10-01' except HG2('2022-04-01') and tagespflege('2022-04-01')
#preprocessed_train_tagespflege, preprocessed_test_tagespflege  = get_data(file_path = path + 'Grosstagespflege.parquet', split_date = '2022-04-01')
#preprocessed_train_hg2, preprocessed_test_hg2  = get_data(file_path = path + 'HGII.parquet', split_date = '2022-04-01')
#preprocessed_train_oh12, preprocessed_test_oh12 = get_data(file_path = path + 'OH12.parquet', split_date = '2021-10-01')
#preprocessed_train_oh14, preprocessed_test_oh14 = get_data(file_path = path + 'OH14.parquet', split_date = '2021-10-01')
#preprocessed_train_kita_hokida, preprocessed_test_kita_hokida = get_data(file_path = path + 'Kita_hokida.parquet', split_date = '2021-10-01')
#preprocessed_train_ef40, preprocessed_test_ef40 = get_data(file_path = path + 'EF40.parquet', split_date = '2021-10-01')
#preprocessed_train_ef40a, preprocessed_test_ef40a = get_data(file_path = path + 'EF40a.parquet', split_date = '2021-10-01')
#preprocessed_train_ef42, preprocessed_test_ef42 = get_data(file_path = path + 'EF42.parquet', split_date = '2021-10-01')
#preprocessed_train_chemie, preprocessed_test_chemie = get_data(file_path = path + 'Chemie.parquet', split_date = '2021-10-01')
#preprocessed_train_erich_brost, preprocessed_test_erich_brost = get_data(file_path = path + 'Erich_brost.parquet', split_date = '2021-10-01')
#preprocessed_train_ch_chemie_single, preprocessed_test_chemie_single = get_data(file_path = path + 'Chemie_singleBuilding.parquet', split_date = '2021-12-01')
#preprocessed_train_ch_chemie_concat, preprocessed_test_chemie_concat = get_data(file_path = path + 'Chemie_concat.parquet', split_date = '2021-12-01')
#preprocessed_train_office_concat, preprocessed_test_office_concat = get_data(file_path = path + 'Office_concat.parquet', split_date = '2021-12-01')
#preprocessed_train_office_single, preprocessed_test_office_single = get_data(file_path = path + 'Office_singleBuilding.parquet', split_date = '2021-12-01')


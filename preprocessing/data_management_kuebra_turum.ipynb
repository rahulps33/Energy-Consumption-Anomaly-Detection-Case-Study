{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dfuls3KkYYM"
      },
      "source": [
        "The data is currently stored as `.xls` files. In this notebook, we will implement some code to manipulate the data as `pandas.Dataframes` and store as more efficient `.parquet` files on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gnNu9xfwkYYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589ad4fd-c214-4378-da21-5f01e6631e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# import any required libraries here\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "#connect google drive and google collab\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "cmC8kgszPmci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkY1kgMkYYR"
      },
      "source": [
        "First, we need to read the `.xls` files into `pandas.Dataframes`. You can use [pandas.read_excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) for this."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the building data \n",
        "# consider the different number of header rows!\n",
        "# only load same header rows \n",
        "\n",
        "def load_data(filepath1, filepath2, col1=None,col2=None):\n",
        "    '''\n",
        "    Merging two .xls-files and saving as a pandas.Dataframe\n",
        "    \n",
        "    Load the two .xls-files, create a unique column and using this unique \n",
        "    column as a header. Furthermore rename the column OBIS Bezeichnung_O_reibung\n",
        "    to 'Date' and change the type to pd.datetime. Next change type of remaining \n",
        "    columns to numeric. Finally merge both .xls-files and save as a pandas.Dataframe\n",
        "\n",
        "    Parameters:\n",
        "    filepath1 (string): Filepath of first .xls-Document\n",
        "    filepath2 (string): Filepath of second .xls-Document \n",
        "    col1 (string): optional parameter, which states the columns to read in file1 \n",
        "    col2 (string): optional parameter, which states the columns to read in file2           \n",
        "\n",
        "    Returns:\n",
        "    df_merged (pd.Dataframe): Merged Dataframe \n",
        "    \n",
        "    '''\n",
        "    #read data \n",
        "    file1 = pd.read_excel(filepath1,usecols=col1, header=1)\n",
        "    file2 = pd.read_excel(filepath2,usecols=col2, header=1)\n",
        "    #dict with OBIS Beschreibung and abbreviation\n",
        "    list_dic = {'Betriebsstunden':'Betriebsstd', 'Fehlerstunden':'Fehlerstd', 'Vorlauftemperatur':'Vorlauftmp',\n",
        "        'Wärmeenergie total':'Wtotal','Temperaturdifferenz': 'TmpDiff','Volumen Kanal 1':'VK1',\n",
        "        'WV+ Arbeit Tarif 1':'WV+T1','WV- Arbeit Tarif 1':'WV-T1','WV+ Arbeit tariflos':'WV+tariflos',\n",
        "        'WV- Arbeit tariflos':'WV-tariflos', 'WV+ Momentanwert Tariflos':'WV+ Momtrflos','Fehler Flags':'FehFlag',\n",
        "        'BV- Arbeit Tarif 1': 'BV-T1','BV+ Arbeit Tarif 1':'BV+T1','Wärmeleistung':'Wleistung',\n",
        "        'BV+ Arbeit tariflos':'BV+tariflos','BV- Arbeit tariflos':'BV-tariflos', 'Durchfluss':'Dfluss',\n",
        "        'Wärmeenergie Tarif 1':'WTarif1', 'P Summe' : 'PSum', 'Rücklauftemperatur':'Rücklauftmp', 'Volumen':'Vol'}\n",
        "    file1.iloc[2].replace(list_dic, inplace=True)\n",
        "    file2.iloc[2].replace(list_dic, inplace=True)\n",
        "    #create unique identifier\n",
        "    for column in file1:\n",
        "      file1[column].iloc[2] = file1[column].iloc[1][0] + '_'+file1[column].iloc[0]+ '_'+file1[column].iloc[2]\n",
        "    for column in file2:\n",
        "      file2[column].iloc[2] = file2[column].iloc[1][0] + '_'+file2[column].iloc[0]+ '_'+file2[column].iloc[2]\n",
        "    #change header to unique identififer  \n",
        "    file1.columns = file1.iloc[2]\n",
        "    file2.columns = file2.iloc[2]\n",
        "    #drop not useful rows\n",
        "    file1 = file1.drop([0,1,2,3])\n",
        "    file2 = file2.drop([0,1,2,3])\n",
        "    #rename col OBIS Bezeichnung_O_reibung to date \n",
        "    file1 = file1.rename(columns = {'O_Beschreibung_OBIS Bezeichnung':'Date'})\n",
        "    file2 = file2.rename(columns = {'O_Beschreibung_OBIS Bezeichnung':'Date'})\n",
        "    #change type of col date to date format  \n",
        "    file1['Date'] = pd.to_datetime(file1['Date'])\n",
        "    file2['Date'] = pd.to_datetime(file2['Date']) \n",
        "    #change type of remaining columns to numeric \n",
        "    file1.iloc[:,1:] = file1.iloc[:,1:].astype('float')\n",
        "    file2.iloc[:,1:] = file2.iloc[:,1:].astype('float')\n",
        "    #sort data\n",
        "    file1 = file1.sort_values(by='Date')\n",
        "    file2 = file2.sort_values(by='Date')\n",
        "    #take difference for specific cols per building\n",
        "    #file1_diff = file1.iloc[:,1:].diff()\n",
        "    #file1_diff.insert(0,'Date', file1['Date'])\n",
        "    #file2_diff = file2.iloc[:,1:].diff()\n",
        "    #file2_diff.insert(0,'Date', file2['Date'])\n",
        "    #merge the two files (considering reseting index, removing overlapping data in date)\n",
        "    df_merged = pd.concat([file1_diff, file2_diff]).reset_index(drop=True)\n",
        "    df_merged = df_merged.drop_duplicates(subset='Date', keep='first')\n",
        "\n",
        "    return df_merged"
      ],
      "metadata": {
        "id": "WWO5qJcoYL0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/case_studies_data/'\n",
        "#excluded empty columns and col Gerätenummer, which is not of interest\n",
        "oh12 = load_data(path+'OH12.xls',path+ 'OH12_01_26-07_19.xls',\"A:K,M:T,V:W,Y,AA\")\n",
        "oh14 = load_data(path+'OH14.xlsx',path+ 'OH14_01_26-07_19.xls',\"A:O,Q,R\")\n",
        "chemie = load_data(path+'Chemie.xls',path+ 'Chemie_01_26-07_19.xls',col1=\"A:H,J,M,P,R\",col2=\"A:H,J:N\")\n",
        "kita= load_data(path+'Kita Hokido.xls',path+ 'Kita Hokido_05_22_20-07_19_22.xls')\n",
        "hg2= load_data(path+'HG II.xls',path+ 'HGII_01_26-07_19.xls',\"A:Q,S,U:W\")\n",
        "tagespflege = load_data(path+'Großtagespflege.xls',path+'Grosstagespflege_04_05-07_19.xls',col1=\"A:T,V,X,Z,AB,AD,AE,AG,AI,AK,AM,AN,AP,AR,AT\", col2=\"A:S,U,W,Y\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szuEQ5YLkqW2",
        "outputId": "c2376a7b-9bd5-4d23-8d3b-7609af6f4bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
            "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
            "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
            "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_vWW27ekYYU"
      },
      "source": [
        "Next, we need to implement a function that takes a `pandas.Dataframe` and a path string as an input and writes the data to disk as a `parquet` file. You can use the [PyArrow library](https://arrow.apache.org/docs/python/parquet.html) for this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDVZl8axkYYV"
      },
      "outputs": [],
      "source": [
        "def write_as_parquet(df, path):\n",
        "    '''\n",
        "    Reading a pandas.Dataframe into a pandas table and saving\n",
        "    it as a parquet file\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas.Dataframe): Dataframe to be saved as .parquet\n",
        "    path (string): Path where the parquet file should be saved           \n",
        "    \n",
        "    '''\n",
        "    pq.write_table(pa.Table.from_pandas(df), path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_as_parquet(oh14,path +'oh14.parquet' )\n",
        "write_as_parquet(oh12,path +'oh12.parquet' )\n",
        "write_as_parquet(chemie,path +'chemie.parquet' )\n",
        "write_as_parquet(kita,path +'kita.parquet' )\n",
        "write_as_parquet(hg2,path +'hg2.parquet' )\n",
        "write_as_parquet(tagespflege,path +'tagespflege.parquet' )"
      ],
      "metadata": {
        "id": "khD1NAAXi9D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsZGM1IEkYYW"
      },
      "source": [
        "Now we need the opposite functionality: a function that reads data from a `.parquet` file on disk and returns it as a `pandas.Dataframe`. Implement this function such that it can take a list of names of column to load as an _optional_ parameter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inr7nryGkYYX"
      },
      "outputs": [],
      "source": [
        "def load_to_pandas(path, columns=None):\n",
        "    '''\n",
        "    Reading a parquet file and saving as a pandas.Dataframe with optional \n",
        "    parameter columns, which states the list of columns to read \n",
        "    \n",
        "    Parameters:\n",
        "    path (string): Path of parquet file \n",
        "    columns (optional parameter): columns to load         \n",
        "\n",
        "    Returns:\n",
        "    df (pd.Dataframe): parquet-file as pandas.Dataframe   \n",
        "    \n",
        "    '''\n",
        "    df = pq.read_table(path, columns).to_pandas()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFFs_sTFkYYY"
      },
      "source": [
        "Great! We can now store data more efficiently on disk and know how to load it again. Store all the data we have as one `.parquet` file per building."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
